---
title: "p8105_hw6_mk4996"
author: "Miho Kawanami"
date: "2025-12-01"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(broom)

set.seed(1)
```

# Problem 1
## Filtering the data 
```{r}
homicide_raw = 
  read_csv("./homicide-data.csv")

homicide_df = 
  homicide_raw |> 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved  = as.numeric(disposition == "Closed by arrest")
  ) |> 

  filter(
    !(city_state %in% c(
      "Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"
    ))
  ) |>

  filter(victim_race %in% c("White", "Black")) |>

  mutate(victim_age = as.numeric(victim_age)) |>

  select(city_state, resolved, victim_age, victim_sex, victim_race)

```

## Baltimore, MD
```{r}
baltimore_df = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD")

fit_baltimore = 
  baltimore_df |> 
  glm(
    resolved ~ victim_age + victim_race + victim_sex,
    data   = _,
    family = binomial()
  )

or_ci_baltimore = 
  fit_baltimore |> 
  broom::tidy(conf.int = TRUE) |> 
  mutate(
    OR       = exp(estimate),
    conf.low = exp(conf.low),
    conf.high = exp(conf.high)
  ) |>
  filter(term == "victim_sexMale") |>
  select(term, OR, conf.low, conf.high)

or_ci_baltimore

```

## Each city
```{r}
city_or_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(
      data,
      \(df) glm(
        resolved ~ victim_age + victim_race + victim_sex,
        data   = df,
        family = binomial()
      )
    ),
    results = map(
      models,
      \(mod) broom::tidy(mod, conf.int = TRUE)
    )
  ) |>
  select(-data, -models) |>
  unnest(results) |>
  mutate(
    OR       = exp(estimate),
    conf.low = exp(conf.low),
    conf.high = exp(conf.high)
  ) |>
  filter(term == "victim_sexMale") |>
  select(city_state, OR, conf.low, conf.high)

city_or_results

city_or_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |>
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  coord_flip() +
  labs(
    x = "City",
    y = "Adjusted OR (Male vs Female)",
    title = "Adjusted OR of case resolution, Male vs Female victims"
  )

```


# Problem 2
## Bootstrap: weather data
```{r}
data("weather_df")

weather_fit = 
  weather_df |>
  lm(tmax ~ tmin + prcp, data = _)

set.seed(1)

boot_results = 
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(
      strap,
      \(df) lm(tmax ~ tmin + prcp, data = df)
    ),
    glance_res = map(models, broom::glance),
    tidy_res   = map(models, broom::tidy)
  ) |>
  select(-strap, -models) |>
  mutate(
    glance_res = map(glance_res, as_tibble),
    tidy_res   = map(tidy_res,   as_tibble)
  ) |>
  unnest(glance_res) |>

  select(.id, r.squared, tidy_res) |>
  unnest(tidy_res) |>
  select(.id, r.squared, term, estimate) |>

  pivot_wider(
    names_from  = term,
    values_from = estimate
  ) |>
  mutate(
    r_sq         = r.squared,
    beta1_over_2 = tmin / prcp
  ) |>
  select(.id, r_sq, beta1_over_2)

boot_results

```

## Density Plot
```{r}
rsquare_plot = boot_results |> 
  ggplot(aes(x = r_sq)) +
  geom_density() +
  labs(
    x = "R-square hat estimates",
    y = "Density",
    title = "Bootstrap Distribution of r² hat")

rsquare_plot

# β¹ ÷ β² distribution
slope_plot = boot_results |> 
  ggplot(aes(x = beta1_over_2)) +
  geom_density() +
  labs(
    x = "β1 hat/ β2 hat estimates",
    y = "Density",
    title = "Bootstrap Distribution of β1 hat/ β2 hat")

slope_plot
```

## 95%CI
```{r}
boot_summary =
  boot_results |>
  summarize(
    r_sq_low  = quantile(r_sq,         0.025),
    r_sq_high = quantile(r_sq,         0.975),
    ratio_low = quantile(beta1_over_2, 0.025),
    ratio_high = quantile(beta1_over_2, 0.975)
  )

boot_summary
```

* **Comments**

The bootstrap distribution of \( \hat{R}^2 \) is narrow and centered around
0.94, indicating that the model’s explanatory power is stable across samples.
The 95% confidence interval for \( R^2 \) is [0.934, 0.947].

In contrast, the distribution of \( \hat{\beta}_1 / \hat{\beta}_2 \) is much wider, showing substantial uncertainty in this ratio. Its 95% confidence interval is [–280, –126].


# Problem 3
## 1. Load and Prepare Birthweight Data
```{r}
birthweight_df = 
  read_csv("./birthweight.csv") 

  # Check missing values
colSums(is.na(birthweight_df))　
  skimr::skim(birthweight_df)

  birthweight_df =
  birthweight_df |>
  mutate(
    babysex = factor(babysex),
    frace   = factor(frace),
    mrace   = factor(mrace),
    malform = factor(malform)
  ) |>
  drop_na()
```

* **Comments**

The dataset contains no missing values, and all variables show complete data. Summary statistics and distributions appear reasonable, with no implausible values or obvious data issues.


## 2. Fit Proposed Birthweight Model (Infant + Maternal Predictors)
```{r}
birthweight_mod =
  birthweight_df |>
  lm(
    bwt ~ gaweeks + blength + babysex +
          momage + parity,
    data = _
  )

summary(birthweight_mod)
```

* **Comments**

I selected gestational age, birth length, infant sex, maternal age, and parity as predictors because these infant and maternal factors are widely known to influence birthweight and provide a biologically reasonable baseline model.


## 3. Diagnostic Plot: Residuals vs Fitted Values
```{r}
birthweight_df |>
  modelr::add_residuals(birthweight_mod) |>
  modelr::add_predictions(birthweight_mod) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Fitted values",
    y = "Residuals",
    title = "Residuals vs Fitted: Proposed Birthweight Model"
  )
```

* **Comments**

The residuals show increasing spread at higher fitted values, suggesting heteroscedasticity. This indicates that the linear model may not fully capture patterns in the data.


## 4. Fit Comparison Models (Model A and Model B)
```{r}
# Model A: gestational age + birth length
birthweight_mod_A =
  birthweight_df |>
  lm(bwt ~ blength + gaweeks, data = _)

# Model B: full three-way interaction
birthweight_mod_B =
  birthweight_df |>
  lm(bwt ~ bhead * blength * babysex, data = _)
```

* **Comments**

Model A represents a simple baseline using only length and gestational age. Model B is intentionally more flexible, including all two- and three-way interactions between head circumference, length, and sex.


## 5. Cross-Validation Setup and Model Training
```{r}
set.seed(1)

cv_df =
  crossv_mc(birthweight_df, 100) |>
  mutate(
    train = map(train, as_tibble),
    test  = map(test,  as_tibble)
  ) |>
  mutate(
    mod_my = map(
      train,
      \(df) lm(
        bwt ~ gaweeks + blength + babysex +
              momage + parity,
        data = df
      )
    ),
    mod_A = map(
      train,
      \(df) lm(bwt ~ blength + gaweeks, data = df)
    ),
    mod_B = map(
      train,
      \(df) lm(bwt ~ bhead * blength * babysex, data = df)
    )
  ) |>
  mutate(
    rmse_my = map2_dbl(mod_my, test, rmse),
    rmse_A  = map2_dbl(mod_A,  test, rmse),
    rmse_B  = map2_dbl(mod_B,  test, rmse)
  )
```

* **Comments**

Cross-validation was performed using 100 random train/test splits via crossv_mc.
For each split, the three models were re-fit to the training data and RMSE was calculated on the test data.
Some warnings occurred for Model B due to rank deficiency in certain splits, which is expected given the high flexibility of the three-way interaction model.


## 6. Compare Cross-Validated RMSE Across Models
```{r}
cv_df |>
  select(starts_with("rmse_")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |>
  mutate(model = fct_inorder(model)) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    x = "Model",
    y = "RMSE",
    title = "Cross-validated RMSE for Birthweight Models"
  )

```

* **Comments**

Based on the cross-validated RMSE, Model B (the interaction model) performed best, showing the lowest prediction error among the three models. My proposed model and Model A had higher and very similar RMSE values, indicating comparable predictive performance but worse accuracy than Model B. Although Model B is the most complex model, in this dataset its flexibility appears to improve prediction.

